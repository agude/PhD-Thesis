\chapter{Data and Simulation Samples}
\label{chatper:data_and_mc_samples}

\section{Data}

The data used in this analysis were collected by the CMS detector in 2012 at a
center-of-mass energy of \rootseight. The LHC delivered 23 \fbinv of integrated
luminosity during the year as seen in \FIG~\ref{fig:2012_luminosity}. This
period was divided into four run eras refered to as 2012A, B, C, and D. During
an era, the LHC run parameters are kept roughly static to allow for consistent
data taking conditions. In between eras, maintenance and beam adjustments were
performed on the LHC in order to deliver higher luminosity.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/2012_lumi.pdf}
    \caption[
        The integrate luminosity delivered and recorded by CMS in 2012
    ]{
        The integrate luminosity delivered and recorded by CMS in 2012. The
        flat periods in May, July, and September correspond to the boundaries
        between the run eras.
    }
    \label{fig:2012_luminosity}
\end{figure}

The data collected by CMS are split into smaller datasets based on the physics
objects contained within the events. This allows analyses to use only one or
two datasets, instead of requiring them to deal with the entirety of the CMS
data (which is many petabytes, and hence too large for most institutes to store
locally). The HLT sorts events into the various datasets based on the triggers
that the event fired. In this manner, an event can end up in multiple datasets
if it fired multiple triggers. This analysis uses the \SingleElectron dataset
which was collected with the HLT trigger \SingleElectronTrigger. These datasets
were reconstructed---converted from raw detector response into physics
objects---in January 2013, in order to make use of the most recent
calibrations derived from the entire 2012 run. A summary of the datasets used
is provided in \TAB~\ref{table:datasets}.

% table:datasets
\input{tables/datasets.tex}

Although there is a \DoubleElectron dataset which uses a trigger designed to
find Z bosons, this analysis uses the \SingleElectron dataset selected with the
\SingleElectronTrigger trigger. The primary motivation behind using this
trigger was to allow a direct comparison with a similar \phistar analysis being
performed by CMS which used \Ztomumu events selected with a single muon
trigger. The single electron trigger requires an electron with $\pt > 27$ which
passes Working Point 80 (\WPEighty). \WPEighty is a set of selection
requirements on lepton isolation and shower shape variables designed to be 80\%
efficient in selecting real electrons. The requirements that make up \WPEighty
are listed in \TAB~\ref{table:wp80}. This trigger had the lowest \pt threshold
of any single electron trigger that was unprescaled run during 2012. To
prescale a trigger means to apply a rate reduction by randomly throwing out a
certain fraction of events in order to keep the total trigger rate manageable;
as this trigger was unprescaled, no events were discarded in this manner.

% table:wp80
\input{tables/wp80.tex}

The events from the \SingleElectron sample are further filtered for quality. A
centrally-produced list of good luminosity segments is used to select only
events in which no part of the detector was malfunctioning or disabled. After
accounting for detector dead time and beam quality, \GoodLumiNumber of
integrated luminosity are used for physics analysis.

\section{Monte Carlo}

This analysis makes use of numerous simulated data samples---colloquially
referred to as Monte Carlo (MC)---in order to estimate backgrounds and signal
yields, derive scale factors, and correct for the effects of bin migration on
the final measurement.

\subsection{Monte Carlo Generators}

There are multiple MC generators, each of which makes its own assumptions about
the behavior of the various particles and interactions it simulates. In
generally, the process of generating a simulated a proton-proton collision
event is broken up into multiple steps. These steps can all be performed by one
program, but in general are handled by several. These steps are summarized
below:

\begin{description}

    \item[Parton distributions:] In order to simulate the collision of two
        protons, a set of PDFs is needed to calculate the probability of
        finding a parton with a specific \BjorkenX{i}. PDFs are provided by
        various theory groups and MC software can generally run with multiple
        different PDF sets.

    \item[Initial state shower:] A shower-initiating parton from each beam of
        protons initiates a sequence of partons branchings (for example, $q \to
        qg$). One parton from each of these showers will go on to interact in
        the hard scattering process, the others will become initial state
        radiation. These shower initiators are given non-zero \pt to account
        for the primordial transverse momentum of the partons in the proton.

    \item[Hard scattering process:] In the hard scattering the two partons
        collide and a set of out going particles is generated based on the
        matrix element level calculations.

    \item[Resonances and decays:] Some of the outgoing particles may be
        resonances (like the \W and \Z) which decay, while others may be stable
        (but which may still radiate, for example, $e \to e \photon$). In
        either case, these particles and their decay chains are simulated in
        detail.

    \item[Multiple interactions and the underlying event:] In addition to the
        hard scattering, there are further interactions from other partons in
        the shower. After the partons from these interaction are removed from
        the protons, what is left are ``beam remnants'' that may have internal
        color and charge. Additionally, these remnants can have non-zero \pt as
        the shower initiators had non-zero \pt. These remnants are still color
        connected to the hard interaction, and must decay accordingly.

    \item[Hadronization:] Bare quarks and gluons can not be observed, so the
        must be combined into colorless objects. The exact manner in which this
        happens is not understood from QCD, and so the different MC software
        suites use different internally developed models to perform
        hadronization.

    \item[Detector Simulation:] After the event is generated and all of the
        particles decayed, the event is run through a simulation of the
        detector which particle interactions with mater are simulated along
        with the response of the detector. This step is almost always done by a
        stand alone program as the job of simulating a detector is very
        different (and almost wholly separable from) simulating a collision and
        the resulting event.

\end{description}

In the MC samples that we use, that parton distributions, initial state shower,
hard scattering, and resonances and decays are handled by \MADGRAPH, \POWHEG,
and \PYTHIAsix, as indicated in \TAB~\ref{table:mc}. All of the samples use
\PYTHIAsix for the underlying event and hadronization. The CMS detector is
simulated using \GEANTfour.

MC programs have multiple different parameters that control their behavior in
each of these steps. As not all of these parameters can be derived from QCD,
and so many of them must be ``tuned'' to best fit the data. The parameters are
correlated and so need to be fit together. One such example of tuned parameters
are the various PDF inputs, which are generally produced by theory groups by
fitting their parton models to collider data. Parameters that control
hadronization and final-state showers are fit using data from \LEP, as it
offers the cleanest environment as the incoming electrons have no color charge.
Parameters that control multiparton interactions and initial state radiation
are fit using earlier LHC data.

\subsection{Monte Carlo Datasets}
\label{ssec:monte_carlo}

All the MC used in this analysis were centrally-generated by the CMS
collaboration. A \DYtoll signal sample and a $\ttbar\text{+jets}$ background
sample were generated with \MADGRAPH \cite{alwall2014}. Diboson ($\Z\Z$,
$\W\Z$, $\W\W$) background samples were generated with \PYTHIAsix
\cite{sjostran2006}. Background samples consisting of $\tbar \W$, $t \W$, and
$\DYtotautau$ were generated using \POWHEG
\cite{nason2004}\cite{alioli2010}\cite{re2011}. A secondary signal MC was also
generated with \POWHEG. The details of these samples are listed in
\TAB~\ref{table:mc}.

All of the MC samples use \PYTHIAsix with the \ZTwoStar tune for modeling the
underlying event and hadronization except for the di-boson samples. The
\ZTwoStar tune was centrally-produced by the CMS collaboration by tuning
\PYTHIAsix to match the 2011 CMS data. \Tauola is used for tau decays in all of
the samples except for \ttbar and \POWHEG $\text{Drell-Yan} \to \ee$ MC samples
\cite{was_2007}.

% table:mc
\input{tables/mc.tex}

After the generation step, MC is sent through a full detector simulation which
uses \GEANTfour \cite{agostinelli2003} to mimic the detector response. This
detector response is reconstructed using the full CMS reconstruction chain to
produce MC files in a format identical to actual data.

MC events have additional simulated minimum-bias events overlaid on top of them
to better match the conditions found in actual running. These minimum-bias
events attempt to simulate what a typical proton-proton interaction looks like
in the detector and are used to mimic pileup. The number of pileup events to be
added is drawn from a distribution decided upon before data is taken and so it
events must be reweighted after the data taking period is over to make the
number of pileup in MC match the same quantity in data. The data distribution
is calculated based on the instantaneous luminosity and the inelastic
proton-proton cross section. The ratio of these measurements is used to
reweight the MC.

\section{Scale Factors}
\label{sec:scale_factors}

The detector response to various signals is not always perfectly simulated in
MC and so the efficiencies of various selection requirements are not the same
in data and MC. In order to correct for this difference, each event in MC is
reweighed with a series of scale factors which are the efficiency of some
selection requirement in data divided by the same efficiency as measured on MC,
as follows:

\begin{equation}
    \label{eq:sf}
    \text{SF} = \frac{\effdata}{\effmc}
\end{equation}

Three scale factors are applied to each event: trigger, reconstruction, and
identification. The trigger scale factors were measured by us and are detailed
in \SEC~\ref{ssec:sf_trigger} while the reconstruction and identification scale
factors were measured centrally by the CMS collaboration. The
centrally-produced values are used as doing so is a requirement of passing the
internal analysis review. The methods used to measure the reconstruction and
isolation scale factors are summarized in \SECS~\ref{ssec:sf_reconstruction}
and \ref{ssec:sf_id} because the papers detailing them are not public

\subsection{Tag and Probe}

Tag and Probe (\TnP) is a minimally-biased method of calculating the efficiency
of some analysis selection requirement. \TnP takes advantage of the well-known
mass and narrow width of the \Z boson to select a set of electrons for which
very few selection requirements have been applied. This is done by finding one
high-quality electron, the tag, and another minimally-biased object, the probe,
that could be an electron, such as a supercluster. The invariant mass of these
objects is computed and if it is near the \Z mass peak, it is very likely that
the probe is also an electron.

Once a set of minimally-biased probe electrons is constructed, the selection
requirement can be applied to them. The efficiency of that requirement is then
the number of probes that passs divided by the total number in the sample as
follows:

\begin{equation}
    \label{eq:eff}
    \eff = \frac{
        \text{n}^{\text{obs.}}_{\text{pass}}
    }{
        \text{n}^{\text{obs.}}_{\text{total}}
    }
\end{equation}

\subsection{Single Electron Trigger}
\label{ssec:sf_trigger}

The efficiency of the HLT trigger used in this analysis,
\SingleElectronTrigger, is measured using \TnP on the primary dataset. The
efficiency is measured in bins of probe \pt and probe $\eta$ with bin
boundaries of \{30, 40, 50, 70, 250\} in \pt and \{-2.1, -2.0, -1.556, -1.442,
-0.8, 0., 0.8, 1.442, 1.556, 2.0, 2.1\} in $\eta$.

Both the tag electron and the probe electron are required to satisfy $|\eta| <
2.1$, $\pt > 30$, and to pass \EGTIGHT requirements. These requirements are the
same as required of the \CentralElectron in the full analysis selection, and
hence the efficiency is measured relative to that selection. The pair must have
an invariant mass such that \MassRange. The tag electron is required to be
matched to an electron that fired the trigger with $\Delta R < 0.3$. There is
no requirement placed on the charge of the electron pair. Events with three or
more electrons that pass these requirements are rejected.

Probes are considered passing if they are also matched to an electron that
fired the trigger with $\Delta R < 0.3$, and failing otherwise. The efficiency
in each bin is the number of passing probes divided by the number of failing
probes, where the number of passing and failing probes is determined by using a
simple count. In an individual event, both electrons are tried as a tag so that
an event may contribute to the efficiency measurement twice if both electrons
pass the tag requirements.

The efficiency is computed in exactly the same way on the \MADGRAPH sample. The
MC events are reweighted for pileup, reconstruction efficiency, and
identification efficiency before the trigger efficiency is measured. The
measured efficiencies for data and MC are listed in
\TABS~\ref{table:trigger_eff_data} and \ref{table:trigger_eff_mc}, respectively.

In the case of the trigger, because either electron could cause the event to
pass, the scale factors can not be computed for each bin, but instead must be computed
for each pair of bins. If only one electron in the event has $\pt > 30$ and
$|\eta| < 2.1$ than the scale factor is simply that given by \EQ~\ref{eq:sf}, but if both
electrons pass the requirements than either could have fired the trigger and so
the scale factor is given by:

\begin{equation} \label{eq:sf_double}
    \text{SF}_{1 \text{ or } 2}
    =
    \frac{
        1 - \left( 1 - \effdata_{0} \right) \left( 1 - \effdata_{1} \right)
    } {
        1 - \left( 1 - \effmc_{0} \right) \left( 1 - \effmc_{1} \right)
    }
\end{equation}

Where $\effdata_{0,1}$ is the efficiency as measured in data for the 0th and
1st electrons, and $\effmc_{0,1}$ is the efficiency as measured in MC.
Equation~\ref{eq:sf_double} is just the probability that one or both of the
electrons fired the trigger divided by the same quantity in MC. This equation
assumes that the probability of one electron firing the trigger is uncorrelated
with the probability of the other electron firing the trigger.

% table:trigger_eff_data
\input{tables/trigger_efficiency_data.tex}

% table:trigger_eff_mc
\input{tables/trigger_efficiency_mc.tex}

\subsection{Electron Reconstruction}
\label{ssec:sf_reconstruction}

Electron reconstruction begins with the assembly of a supercluster in ECAL and
ends with the matching of a supercluster to a track in the tracker. The details
of electron reconstruction are described in
\SEC~\ref{sec:electron_reconstruction}. The efficiency of an electron with $\pt
> 20 \GeV$ depositing enough energy in ECAL to be reconstructed into a
supercluster is very high, although the exact efficiency must be measured in
MC as there is no more basic object with which to perform \TnP to measure it in
data. Failure to form superclusters is generally due to dead crystals in ECAL,
which are accounted for in the detector response simulation. Scale factors for
matching a track given that a supercluster has already been found were measured
centrally by the CMS collaboration using \TnP \cite{gsf_scale_factors_2013}. A
summary of their method follows.

The events used to measure the reconstruction scale factors are selected with the
dedicated electron \TnP Trigger: \TnPTrigger. This trigger requires one
electron with $\pt > 20 \GeV$ which must also pass very tight isolation and ID
requirements while requiring only a low energy ($\et > 4 \GeV$) supercluster as
the other leg. The trigger rate is kept down by requiring that the invariant
mass of these two objects is greater than $50 \GeV$.

The events selected by the trigger are further required to pass a set of
selection requirements. The tag electron is required to pass \EGTIGHT, have
$\pt > 25 \GeV$, and $|\eta| < 2.5$. Electrons are rejected if they fall in the
seam between EB and EE ($1.4442 < |\eta| < 1.566$). The tag must also be
matched to the tight leg of the \TnP trigger. The probe supercluster has
minimal requirements applied; it is required to have tracker isolation $<
0.15$. For the MC sample, the tag is required to be matched to a generator
level electron with $\Delta R < 0.2$. Additionally, the event is required to
have low particle flow missing energy $\PFMET < 20 \GeV$ in order to reject
poorly reconstructed events.

The events were binned in terms of probe's $\pt$ and $\eta$ as well as whether
the probe passed or failed. In each bin, the \mee distribution was constructed
and a template consisting of the sum of a Gaussian smeared \Ztoee MC sample and
an exponential background was fitted. The number of events predicted by the
signal fit on the passing sample, failing sample, and sum of the two samples
was used to get the efficiency. A similar process was performed on MC, although
instead of a fit a simple counting of passing events was performed (as there is
no background in MC). The resulting scale factors are given in
\TAB~\ref{table:gsf_scale_factor}.

% table:gsf_scale_factor
\input{tables/gsf_scale_factors.tex}

\subsection{Electron Identification}
\label{ssec:sf_id}

Not all electrons which are reconstructed pass the ID criteria used in this
analysis, specifically \EGMEDIUM and \EGTIGHT, the details of which are covered
in \SEC~\ref{sec:cut_based_id}. The efficiency of going from a
reconstructed electron to one which passes the identification criteria is
measured centrally by the CMS collaboration using \TnP \cite{cms_an_2014-055}.
A summary of their method follows.

The events used for this measurement were selected using two triggers: \\
\TnPTrigger, which is described above, and \TnPTriggerSecond, which requires
one electron with $\pt > 17 \GeV$ and tight isolation and ID requirements while
also requiring a reconstructed second electron (as opposed to a supercluster as
required by the first trigger) with $\pt > 8 \GeV$. It further requires a
dielectron invariant mass of $\mee > 50 \GeV$.

The tag electrons are required to pass \EGTIGHT, have $\pt > 25 \GeV$, and
$|\eta| < 2.5$; they are rejected if they fall in the seem between EB and EE
($1.4442 < |\eta| < 1.566$). The tag is not required to match the trigger.
Probe electrons have the same $\eta$ requirements as tags, but are only
required to have $\pt > 10 \GeV$. Passing probes pass the ID criteria under
investigation, failing probes fail the ID criteria. The invariant mass of the
tag and probe pair is required to be near the \Z mass peak (\MassRange). The
electrons are required to have charges of opposite sign. In MC, the probe is
only required to be matched to a generator electron with $\Delta R < 0.2$.

The efficiencies are then calculated by fitting the \mee distributions using a
template constructed with a \Ztoee MC sample and an exponential background. The
three categories (passing probes, failing probes, and all probes) are then
simultaneously fit with this template and the number of fitted signal events is
used to derive an efficiency. A simple count of events is used for the MC
efficiency instead of a fit. The resulting scale factors are given in
\TABS~\ref{table:tight_scale_factor} and \ref{table:medium_scale_factor}.

% table:tight_scale_factor
\input{tables/tight_scale_factors.tex}

% table:medium_scale_factor
\input{tables/medium_scale_factors.tex}
