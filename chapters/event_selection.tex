\chapter{Event Selection}
\label{event_selection_chapter}

This chapter details the requirements used to select events for the analysis.
It also covers the data used and the Monte Carlo (MC) used. The final state we
are considering is \Ztoee.

\section{Acceptance}

The acceptance region is a definition of what events, assuming that there are
no limitations due to the detector design, we include in our analysis. It is
essential to define an acceptance region so that our final result can be
compared to other measurements and to theory without forcing other groups to
attempt to correct for our experimental effects. The acceptance region defines
what sort of physics results we can make statements about, and also determines
the value of the effective cross-section of the \Z.

Our acceptance is defined by the kinematics of the two electrons and the mass
of the Z boson. One of the electrons, called the \CentralElectron, is required
to have $\pt > 30 \GeV$ and to be within the central region of the detector
(hence the name) with $|\eta| < 2.1$. The other electron, called the
\ExtendedElectron, has looser requirements; it must have $\pt > 20 \GeV$ and is
not required to be as central with $|\eta| < 2.4$. The requirements on the
\CentralElectron were selected in conjunction with the \Ztomumu measurement of
\phistar at CMS so that that measurement and this one could be easily combined
into a joint measurement. The pseudorapidity limit was selected to match the
most efficient region of CMS's single muon trigger, while the transverse
momentum threshold was dictated by threshold on single electron trigger. The
pseudorapidity limit on the \ExtendedElectron was chosen to keep all of the
electrons within the region covered by the tracker (which allows a better
angular measurement than ECAL alone), while the transverse momentum threshold
was selected because for $\pt < 20 \GeV$ the rate of fake electrons increases.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figures/dielectron_mass_7tev.png}
    \caption{The spectrum of \ee events as measured by CMS in 2010.}
    \label{fig:ee_spectrum}
\end{figure}

There are other particles, like the \jpsi, that decay to \ee pairs as shown in
\FIG~\ref{fig:ee_spectrum}. Fortunately, none of these other particles are near
the \Z in mass, and so we can eliminate them from our acceptance by requiring a
mass near the \Z peak. We therefore define our mass window around the nominal
\Z mass of $91 \GeV$, extending from $60 \GeV$ to $120 \GeV$.

\section{Data and Monte Carlo}

\subsection{Data}

The data used in this analysis were collected by the CMS detector in 2012 at a
center of mass energy of \rootseight. The LHC delivered 23 \fbinv of integrated
luminosity during the year as seen in \FIG~\ref{fig:2012_luminosity}. This
period was divided into four run eras called 2012A, B, C, and D. During an era,
the LHC run parameters are kept roughly static to allow for consistent data
taking conditions. In between eras, maintenance and minor upgrades are
performed on the LHC in order to deliver higher luminosity.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{figures/2012_lumi.pdf}
    \caption{ The integrate luminosity per day delivered and recorded by CMS in
        2012. The flat periods in May, July, and September correspond to the
        boundaries between the run eras. }
    \label{fig:2012_luminosity}
\end{figure}

The data collected by CMS are split into smaller datasets based on the physics
objects contained within the events. This allows analyses to use only one or
two datasets, instead of requiring them to deal with the entirety of the CMS
data (which is petabyte scale, and hence too large for most institutes to store
locally). The HLT sorts events into the various datasets based on the triggers
that the event fired. In this manner, and event can end up in multiple datasets
if it fired multiple triggers. This analysis uses the \SingleElectron dataset
which was collected with the HLT trigger \SingleElectronTrigger. These datasets
were reconstructed---converted from raw detector response into physics
objects---in January, 2013, in order to make use of the most recent
calibrations derived from the entire 2012 run. A summary of the datasets used
are listed in \TAB~\ref{table:datasets}.

\begin{table}[h]
\centering
\begin{center}
    \begin{tabular}{ | l | c | c |}
    \hline
    Dataset Name                          & Run Range      & Luminosity       \\ \hline
    /SingleElectron/Run2012A-22Jan2013-v1 & 190456--193621 & $889.362 \pbinv$ \\ \hline
    /SingleElectron/Run2012B-22Jan2013-v1 & 193833--196531 & $4.429 \fbinv$   \\ \hline
    /SingleElectron/Run2012C-22Jan2013-v1 & 198022--203742 & $7.152 \fbinv$   \\ \hline
    /SingleElectron/Run2012D-22Jan2013-v1 & 203777--208686 & $7.318 \fbinv$   \\ \hline
    \end{tabular}
\end{center}
\caption{
    The datasets used in this analysis.
}
\label{table:datasets}
\end{table}

Although there is a \DoubleElectron dataset which uses a trigger designed to
find Z bosons, this analysis uses the \SingleElectron dataset selected with the
\SingleElectronTrigger trigger. The primary motivation behind using this
trigger was to allow a direct comparison with a similar \phistar analysis being
performed by CMS which used \Ztomumu events selected with a single muon
trigger. The single electron trigger requires an electron with $\pt > 27$ which
passes Working Point 80 (\WPEighty), a set of selection requirements on lepton
isolation and shower shape designed to be 80\% efficient on electrons. The
requirements that make up \WPEighty are listed in \TAB~\ref{table:wp80}. This
trigger had the lowest \pt threshold of any single electron trigger that was
unprescaled run during 2012. To prescale a trigger means to apply a rate
reduction by randomly throwing out a certain fraction of events in order to
keep the total trigger rate manageable; as this trigger was unprescaled, no
events were discarded in this manner.

\begin{table}[h]
\centering
\begin{center}
    \begin{tabular}{ | c | c c |} \hline
        Value                      & EB     & EE     \\ \hline
        $|\eta| <$                 & 1.4791 & 2.65   \\
        $\pt >$                    & 27     & 27     \\
        $\sigmaietaieta <$         & 0.1    & 0.03   \\
        $\ECALISO / \et <$         & 0.15   & 0.1    \\
        $\HOverE <$                & 0.1    & 0.05   \\
        $\HCALISO / \et <$         & 0.1    & 0.1    \\
        Pixel Matching $\ge$       & 1      & 1      \\
        $|\ooeoop| <$              & 0.05   & 0.05   \\
        $|\Delta \eta| <$          & 0.007  & 0.007  \\
        $|\Delta \phi| <$          & 0.06   & 0.03   \\ \hline
    \end{tabular}
\end{center}
\caption{
    The selection requirements for the \SingleElectronTrigger trigger for
    electrons which end up in the barrel region or the endcap region of ECAL.
    The variables used are detailed in \SEC~\ref{sec:electron_variables}.
}
\label{table:wp80}
\end{table}

The events from the \SingleElectron sample are further filtered for quality. A
centrally produced list of good luminosity segments is used to select only
events in which the part of the detector was malfunctioning or disabled. After
accounting for detector dead time and beam quality, \GoodLumiNumber of
integrated luminosity are used for physics analysis.

\subsection{Monte Carlo}
\label{ssec:monte_carlo}

This analysis makes use of numerous simulated data samples---colloquially
referred to as Monte Carlo (MC)---in order to estimate backgrounds and signal
yields, derive scale factors, and correct for the effects of bin migration on
the final measurement. All the MC used in this analysis were central generated
by the CMS collaboration. A \DYtoll signal sample and a $\ttbar\text{+jets}$
background sample were generated with \MADGRAPH \cite{alwall2014}. Diboson
($\Z\Z$, $\W\Z$, $\W\W$) background samples and a \DYtotautau background sample
were generated with \PYTHIA \cite{sjostran2006}. Background samples consisting
of $\tbar \W$ and $t \W$ were generated using \POWHEG
\cite{nason2004}\cite{alioli2010}\cite{re2011}. The details of these samples
are listed in \TAB~\ref{table:mc}.

\begin{table}[h]
\centering
\begin{center}
    \begin{tabular}{ | l | l c c |}
    \hline
    Process                                &  Generator & $\sigma$, pb  & Events $(\times 10^{6})$ \\ \hline
    \DYtoll                                &  \MADGRAPH & 3531.9 (NNLO) & 30.460 \\
    \DYtotautau                            &  \PYTHIA   & 1966.7 (\TODO{??})       & 3.297  \\
    \ttbar                                 &  \MADGRAPH & 23.64         & 3.984  \\
    $t \rightarrow \W+b \rightarrow X$     &  \POWHEG   & 11.1          & 0.498  \\
    $\tbar \rightarrow \W+b \rightarrow X$ &  \POWHEG   & 11.1          & 0.493  \\
    $\W\W$                                 &  \PYTHIA   & 54.84         & 10.000 \\
    $\W\Z$                                 &  \PYTHIA   & 33.21         & 10.000 \\
    $\Z\Z$                                 &  \PYTHIA   & 17.7          & 9.800  \\ \hline
    \end{tabular}
\end{center}
\caption{
    Summary of the MC samples used in this analysis. All cross sections are NLO
    unless otherwise stated. \TODO{Is pythia NLO?}
}
\label{table:mc}
\end{table}

After the generation step, MC is sent through a full detector simulation which
uses \GEANTfour \cite{agostinelli2003} to mimic the detector response. This
detector response is reconstructed using the full CMS reconstruction chain to
produce MC files that are in a format identical to actual data.

MC events have actual data events overlaid on top of them to better match the
conditions found in actual running. These overlaid events come from the minimum
bias dataset, which is selected with a minimum of conditions on order to select
events that are "typical" of proton-proton collisions. Additional events are
overlaid in order to simulate pileup. The number of pileup events overlaid is
drawn from a distribution that is expected to match the distribution seen in
data. Of course it is not possible to perfectly predict the data distribution
and so a reweighting technique is used to match the MC distribution to the
data. The distribution in MC is compared to that in data and a weight is
assigned to each MC event to force the distributions to match.

\section{Object Selection}

\subsection{Electron Selection}
\label{ssec:electron_selection}

The requirements used to select electrons offline are selected to be tighter
than the requirements used in the trigger in order to make calculating the
various efficiencies easier. For an event to be considered it must have at
least two electrons with $\pt > 20 \GeV$ and $|\eta| < 2.4$ which are the
looser bounds used in our acceptance. If three or more electrons pass this
initial requirement, only the two highest \pt electrons are considered. One of
these electrons must be within $|\eta| < 2.1$ and it must also have $\pt > 30
\GeV$.

There are several centrally defined ``cut based identification'' requirements
used in offline electron selection at CMS. We use two of these, referred to as
\EGMEDIUM and \EGTIGHT, with \EGTIGHT having stricter requirements than
\EGMEDIUM. The exact definition of these requirements are given in
\TAB~\ref{table:eg_cuts}.

\begin{table}[h]
\centering
\begin{center}
    \begin{tabular}{ | c | c  c | c  c |} \hline
        \multirow{2}{*}{Variable}     & \multicolumn{2}{c |}{\EGTIGHT}    & \multicolumn{2}{c |}{\EGMEDIUM} \\
                                      & EB        & EE        & EB        & EE \\ \hline
        $\detain <$                   & 0.004     & 0.005     & 0.004     & 0.007 \\
        $\dphiin <$                   & 0.03      & 0.02      & 0.06      & 0.03 \\
        $\sigmaietaieta <$            & 0.01      & 0.03      & 0.01      & 0.03 \\
        $\HOverE <$                   & 0.12      & 0.10      & 0.12      & 0.10 \\
        $d_{0} <$                     & 0.02      & 0.03      & 0.02      & 0.02 \\
        $d_{z} <$                     & 0.1       & 0.1       & 0.1       & 0.1 \\
        $|\ooeoop| <$                 & 0.05      & 0.05      & 0.05      & 0.05 \\
        $\pvtx <$                     & $10^{-6}$ & $10^{-6}$ & $10^{-6}$ & $10^{-6}$ \\
        $\nmiss \le$                  & 0         & 0         & 1         & 1 \\
        $\PFISO / \pt^{\text{e}} <$   & 0.10      & 0.10      & 0.15      & 0.15 \\ \hline
    \end{tabular}
\end{center}
\caption{
    Identification and isolation requirements for \EGTIGHT and \EGMEDIUM
    requirements in the ECAL barrel (EB) and ECAL endcap (EE).
    The variables used are detailed in \SEC~\ref{sec:electron_variables}.
}
\label{table:eg_cuts}
\end{table}

A \CentralElectron is required to pass \EGTIGHT and to be matched to one of the
electrons that passed \SingleElectronTrigger with $\Delta R < 0.3$. The other
electron must pass \EGMEDIUM. If both electrons are \CentralElectrons, then
only one of them need pass \EGTIGHT, but this same electron must also match the
trigger; the other \CentralElectron need only pass \EGMEDIUM.

No charge requirements are applied as even the same sign electron sample is
dominated by actual \Z decays as demonstrated by the large peak in the \mee
distribution.

\TODO{Plots: Electron plots: $\eta$, \pt}

\subsection{\Z Selection}

The \Z boson decays too quickly to leave any direct signal in the detector, so
it is reconstructed from its decay products: the two electrons whose selection
is described in \SEC~\ref{ssec:electron_selection}. In events where the two
electrons pass the selection, the \Z is constructed by taking the sum of the
electrons four-vectors. The resulting invariant mass of the \Z must be within
the region set by the acceptance (\MassRange).

\TODO{Plots: Z plots: \mee, Y, $Z_{\pt}$}

\section{Background Estimation}

The distribution of background events is estimated using the MC samples
discussed in \SEC~\ref{ssec:monte_carlo}, with the exception of QCD related
backgrounds, which are computed using data. The various MC samples are
reweighed so that they have equivalent luminosity to the data and the event
selection requirements are applied to them. The number of events that survive
the selection are taken as the number in our data, and are subtracted off from
the data events.

\begin{table}[h]
\centering
\begin{center}
    \begin{tabular}{ | l | r | r |}
    \hline
    Process           & of total & of background \\ \hline
    Signal: $\DYtoee$ &  99.53\% & N.A. \\ \hline
    \ttbar            &   0.14\% & 30.9\% \\ \hline
    $\Z\Z$            &   0.13\% & 27.2\% \\ \hline
    $\W\Z$            &   0.12\% & 25.9\% \\ \hline
    $\W\W$            &   0.03\% &  6.8\% \\ \hline
    $\DYtotautau$     &   0.03\% &  5.8\% \\ \hline
    $t\W + \tbar\W$   &   0.02\% &  3.4\% \\ \hline
    \end{tabular}
\end{center}
\caption{
    Data sample composition as a percentage of the total, as well as the
    backgrounds as a percentage of all background events.
    \TODO{Add QCD}
}
\label{table:bg_percentages}
\end{table}

Overall, the selection requirements discussed in the previous sections leave a
very pure sample of \Z boson events. 99.5\% of events are signal. The dominant
backgrounds are the diboson backgrounds and \ttbar. We define \Z bosons
produced in association another weak boson as a background because of their
different production mechanism as opposed to single \Z boson events. The
fraction of events from each of the considered backgrounds is listed in
\TAB~\ref{table:bg_percentages}

\subsection{QCD Background Estimation}

The centrally produced QCD MC does not have enough events to make an accurate
estimation of the QCD background in this analysis so instead a data driven
method is employed. The same requirements as discussed in
\SEC~\ref{ssec:electron_selection} are applied to both the data and the MC
samples with the additional requirement that both electrons must have the same
sign. The MC samples are then weighted to match the luminosity of the data and
their events are subtracted. The remaining events still have a large
contribution from the \Z (see \FIG~\ref{fig:same_sign_z_peak}) and so we reject
events that fall within the \mee acceptance region of \MassRange.
\TODO{Are we just taking this number, or do we fit and integrate?}

\TODO{Same sign \mee peak plot before and after subtraction.
\label{fig:same_sign_z_peak}}
